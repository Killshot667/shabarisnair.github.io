---
title: "Sprinklr"
collection: experiences
category: professional
excerpt: |
    <div style='display: flex; justify-content: space-between; align-items: center;'>
    <span><strong>AI Product Engineer - Voice Team</strong></span>
    <span>July 2023 - Present</span>
    </div>

    - Worked on optimization and maintainance of deployment pipelines of Text-to-Speech (TTS) and
    Automatic Speech Recognition (ASR) services.
    - Owned the bulk of TTS client deployments (70+), including their maintainance, bug fixes, and alerts.
    - Trained and deployed the first multilingual TTS model which is the primary inhouse TTS currently being used.
    - Core member of the Voice Research team where I worked on problems like contextual biasing in ASR systems, zero-shot adaption
    of ASR using prompting, and confidence estimation, and published papers in the same.
    - Spearheaded the ASR latency optimization track and implemented infrastructural solutions to boost latency of existing models by 3-4 times.
    - Peformed independent research on streaming speech recognition to bring down the latency of prior solution by 50%.
    - Trained and deployed the first multilingual TTS model which is the primary inhouse TTS currently being used.
# permalink: /experiences/work-2
# date: 2022-12-03
# venue: "Briefings in Bioinformatics'23"
# status: "Published"
# slidesurl: 'http://academicpages.github.io/files/slides2.pdf'
# paperurl: 'https://killshot667.github.io/shabarisnair.github.io/files/concept.pdf'
# citation: 'Vandana Bharti, Shabari S Nair, Akshat Jain, Kaushal Kumar Shukla, Bhaskar Biswas'
---

<div style="display: flex; justify-content: space-between; align-items: center;">
  <span><strong>AI Product Engineer Intern</strong></span>
  <span>May 2022 - July 2022</span>
</div>

- Performed independent research on the topic of Structured Sentiment Analysis. [[refer]](https://aclanthology.org/2022.semeval-1.180.pdf)
- Implemented modular (splitting the main task into sub tasks) and dependency-graph (end-to-end model) based models
- Optimized the dependency-graoh based model using LLMs like XLM-RoBERTa for better word embeddings and data augmentation, leading to a 15% average improvement over the baseline across 7 datasets in 5 languages
- Adapted the model to work for Asian languages such as Mandarin, as well as for sarcastic texts, via Multitask learning